'Repository Structure: \'\n\' ├── top-3 socall medai plat forms.png\n├── .gitignore\n├── README.md\n├── app\n│   ├── main.py\n│   ├── over_view.py\n│   ├── satisfaction_analytics.py\n│   ├── other_module.py\n│   ├── experience_analytics.py\n│   ├── __pycache__\n│   ├── user_engagment.py\n│   └── visualization.py\n└── note_book\n    ├──  __init__.py\n    ├── Basic_metric.ipynb\n    ├── Data_cleaing.ipynb\n    ├── user_behaviour.ipynb\n    ├── script.py\n    ├── __pycache__\n    ├── user_engagement.ipynb\n    └── User Overview analysis.ipynb\n \'\n\' ### BEGIN File: README.md ### \nCommit History: \n{"insertions": [2], "deletions": [0], "lines": [2], "committed_datetime": ["2023-12-13 17:52:15"], "commit_count": 1} \n Content: \n# Week-1-User-Analytics-in-the-Telecommunication-Industry\n\n \n### END File: README.md ### \n\n### BEGIN File: connection/script.py ### \nCommit History: \n{"insertions": [1, 2], "deletions": [1, 2], "lines": [2, 4], "committed_datetime": ["2023-12-13 17:52:15", "2023-12-14 20:20:09"], "commit_count": 2} \n Content: \nfrom sqlalchemy import create_engine\nimport pandas as pd\n\ndef create_conn():\n    engine = None\n    try:\n        # Create an engine that connects to PostgreSQL server\n        engine = create_engine(\'postgresql://postgres:telecom@localhost:5432/telecom\')\n        print("Connection successful")\n    except Exception as error:\n        print(error)\n\n    return engine\n\ndef fetch_data(engine, table_name):\n    df = None\n    try:\n        # Execute a query and fetch all the rows into a DataFrame\n        df = pd.read_sql_query(f"SELECT * FROM {table_name};", engine)\n    except Exception as error:\n        print(error)\n\n    return df\n \n### END File: connection/script.py ### \n\n### BEGIN File: connection/Data_cleaing.ipynb ### \nCommit History: \n{"insertions": [762, 542, 687, 26, 36], "deletions": [2, 0, 661, 0, 10], "lines": [764, 542, 1348, 26, 46], "committed_datetime": ["2023-12-13 21:29:21", "2023-12-13 21:37:53", "2023-12-14 14:22:22", "2023-12-14 20:20:09", "2023-12-14 22:59:19"], "commit_count": 5} \n Content: \nimport  script\n\nimport pandas as pd\n## importing data from the database \nengine=script.create_conn()\n\ndata=script.fetch_data(engine, "xdr_data")\ndata\n## identifiying the missing fileds in each colomn\nmissing_fields = data.isnull().sum()\n\nprint(missing_fields)\ncolumn_data_types = data.dtypes\n\nprint(column_data_types)\nfloat_columns = data.select_dtypes(include=[\'float64\'])\n\nfloat_columns\nfilled_data = float_columns.fillna(float_columns.mean())\n\nprint(filled_data)\nmissing_fields = filled_data.isnull().sum()\n\nprint(missing_fields)\nimport pandas as pd\n\n\n\n# Create a DataFrame with non-float columns\n\nnon_float_columns = data.select_dtypes(exclude=[\'float64\'])\n\n\n\n\n\ncleand_data = pd.concat([filled_data, non_float_columns], axis=1)\n\n\n\nprint(cleand_data)\n## Damping the cleand data back to the data base\nfrom sqlalchemy import create_engine\n\n\n\n# Create an engine that connects to PostgreSQL server to damp the clean data\n\nengine = create_engine(\'postgresql://postgres:telecom@localhost:5432/telecom\')\n\n\n\n\n\ncleand_data.to_sql(\'xdr_cleaned_data\', engine, if_exists=\'replace\', index=False)\nfrom sqlalchemy import create_engine\n\n\n\n# Create an engine that connects to PostgreSQL server to damp the clean data\n\nengine = create_engine(\'postgresql://postgres:telecom@localhost:5432/telecom\')\n\n\n\n\n\nfloat_columns.to_sql(\'xdr_float_data\', engine, if_exists=\'replace\', index=False) \n### END File: connection/Data_cleaing.ipynb ### \n\n### BEGIN File: connection/User Overview analysis.ipynb ### \nCommit History: \n{"insertions": [38], "deletions": [8], "lines": [46], "committed_datetime": ["2023-12-14 20:20:09"], "commit_count": 1} \n Content: \nimport  script\n## importing data from the database \nengine=script.create_conn()\n\ndata=script.fetch_data(engine, "xdr_data")\ndata\ncolumn_names_list = data.columns.tolist()\n\nprint(column_names_list)\n## prining the first  20 handset type\nprint(data[\'Handset Type\'].head(20))\n## pringing the top 10 handset type\ntop_10_handsets = data[\'Handset Type\'].value_counts().head(10)\n\nprint(top_10_handsets)\n## prining the top 3 manufactures\ntop_3_manufactures = data[\'Handset Manufacturer\'].value_counts().head(3)\n\nprint(top_3_manufactures)\n## prining the top 5 handset per manfacturer \n\n\ntop_3_manufacturers = data[\'Handset Manufacturer\'].value_counts().head(3).index\n\nfiltered_data = data[data[\'Handset Manufacturer\'].isin(top_3_manufacturers)]\n\n\n\n\n\ntop_5_handsets_per_manufacturer = filtered_data.groupby(\'Handset Manufacturer\')[\'Handset Type\'].value_counts().groupby(level=0, group_keys=False).nlargest(5)\n\nprint(top_5_handsets_per_manufacturer)\n \n### END File: connection/User Overview analysis.ipynb ### \n\n### BEGIN File: connection/user_behaviour.ipynb ### \nCommit History: \n{"insertions": [503], "deletions": [1], "lines": [504], "committed_datetime": ["2023-12-14 21:46:19"], "commit_count": 1} \n Content: \nimport  script\n## fetching the cleaned data from the database\nengine=script.create_conn()\n\ncleaned_data=script.fetch_data(engine, "xdr_cleaned_data")\n# 1. \n\nsessions_per_user = cleaned_data.groupby(\'MSISDN/Number\')[\'Bearer Id\'].nunique()\nsessions_per_user\nimport pandas as pd\n\ncleaned_data[\'Start\'] = pd.to_datetime(cleaned_data[\'Start\'])\n\ncleaned_data[\'End\'] = pd.to_datetime(cleaned_data[\'End\'])\n\ncleaned_data[\'Session Duration\'] = (cleaned_data[\'End\'] - cleaned_data[\'Start\']).dt.total_seconds()\n\ntotal_session_duration = cleaned_data.groupby(\'MSISDN/Number\')[\'Session Duration\'].sum()\ntotal_session_duration\n## total upload and download data per user\nimport pandas as pd\n\n\n\n\n\ntotal_download_per_user = cleaned_data.groupby(\'MSISDN/Number\')[\'Total DL (Bytes)\'].sum()\n\ntotal_upload_per_user = cleaned_data.groupby(\'MSISDN/Number\')[\'Total UL (Bytes)\'].sum()\n\n\n\n# Merge the results into a single DataFrame\n\ntotal_data_per_user = pd.DataFrame({\n\n    \'Total Download (Bytes)\': total_download_per_user,\n\n    \'Total Upload (Bytes)\': total_upload_per_user\n\n}).reset_index()\ntotal_data_per_user\n## total data volume in bytes during the session for each application\ncolumn_headers = cleaned_data.columns\n\ncolumn_headers\nimport pandas as pd\n\n\n\n\n\ndownload_columns = [\'Email DL (Bytes)\', \'Youtube DL (Bytes)\', \'Netflix DL (Bytes)\', \'Gaming DL (Bytes)\', \'Other DL (Bytes)\']\n\nupload_columns = [\'Email UL (Bytes)\', \'Youtube UL (Bytes)\', \'Netflix UL (Bytes)\', \'Gaming UL (Bytes)\', \'Other UL (Bytes)\']\n\n\n\n# Calculate total data volume per application per user\n\ntotal_data_per_user_per_app = cleaned_data.groupby(\'MSISDN/Number\')[download_columns + upload_columns].sum().reset_index()\n\ntotal_data_per_user_per_app  \n### END File: connection/user_behaviour.ipynb ### \n\n### BEGIN File: connection/Basic_metric.ipynb ### \nCommit History: \n{"insertions": [731], "deletions": [54], "lines": [785], "committed_datetime": ["2023-12-15 00:19:34"], "commit_count": 1} \n Content: \nimport  script\nengine=script.create_conn()\n\nfloat_data=script.fetch_data(engine, "xdr_float_data")\nfloat_data\n## mean, median, mode is done for the float coloumn\nimport pandas as pd\n\n\n\n\n\n# Exclude \'Bearer Id\', \'IMSI\', \'IMEI\'\n\nselected_columns =float_data.drop([\'Bearer Id\', \'IMSI\', \'IMEI\', \'MSISDN/Number\', \'Start ms\', \'End ms\'], axis=1)\n\n\n\nresults = pd.DataFrame(index=[\'mean\', \'median\', \'mode\', \'variance\', \'std_dev\', \'range\', \'skewness\', \'kurtosis\'])\n\n\n\nfor column in selected_columns:\n\n    mean = selected_columns[column].mean()\n\n    median = selected_columns[column].median()\n\n    mode = selected_columns[column].mode()[0]\n\n    variance = selected_columns[column].var()\n\n    std_dev = selected_columns[column].std()\n\n    data_range = selected_columns[column].max() - selected_columns[column].min()\n\n    skewness = selected_columns[column].skew()\n\n    kurtosis = selected_columns[column].kurtosis()\n\n\n\n    results[column] = [mean, median, mode, variance, std_dev, data_range, skewness, kurtosis]\n\n\n\n# Display the results DataFrame\n\n\n\nresults\nfloat_data.head(10)\nimport matplotlib.pyplot as plt\n\n\n\n# Select columns for analysis\n\ncolumns_for_analysis = ["Dur. (ms)", "Avg RTT DL (ms)", "Avg RTT UL (ms)", "Avg Bearer TP DL (kbps)", "Total UL (Bytes)"]\n\n\n\n# Plot histograms for each selected column\n\nfor column in columns_for_analysis:\n\n    plt.figure(figsize=(8, 5))\n\n    plt.hist(float_data[column].dropna(), bins=30, edgecolor=\'black\', color=\'skyblue\')\n\n    plt.title(f\'Histogram of {column}\')\n\n    plt.xlabel(column)\n\n    plt.ylabel(\'Frequency\')\n\n    plt.show()\n\nimport seaborn as sns\n\n\n\n# Select columns for analysis\n\ncolumns_for_analysis = ["Total UL (Bytes)"]\n\n\n\n# Plot box plots for each selected column\n\nfor column in columns_for_analysis:\n\n    plt.figure(figsize=(8, 5))\n\n    sns.boxplot(y=column, data=float_data, color=\'skyblue\')\n\n    plt.title(f\'Box Plot of {column}\')\n\n    plt.show()\n\nimport pandas as pd\n\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n\n\n# Assuming \'float_data\' is your DataFrame\n\n# Select relevant columns for analysis\n\ncolumns_for_analysis = ["Social Media DL (Bytes)", "Social Media UL (Bytes)", "Google DL (Bytes)", "Google UL (Bytes)",\n\n                         "Email DL (Bytes)", "Email UL (Bytes)", "Youtube DL (Bytes)", "Youtube UL (Bytes)",\n\n                         "Netflix DL (Bytes)", "Netflix UL (Bytes)", "Gaming DL (Bytes)", "Gaming UL (Bytes)",\n\n                         "Other DL (Bytes)", "Other UL (Bytes)", "Total DL (Bytes)", "Total UL (Bytes)"]\n\n\n\n# Create a new DataFrame with selected columns\n\nselected_data = float_data[columns_for_analysis]\n\n\n\n# Calculate the correlation matrix\n\ncorrelation_matrix = selected_data.corr()\n\n\n\n# Plot a heatmap for visualization\n\nplt.figure(figsize=(12, 10))\n\nsns.heatmap(correlation_matrix, annot=True, cmap=\'coolwarm\', fmt=".2f", linewidths=.5)\n\nplt.title(\'Correlation Heatmap - Applications vs Total DL+UL Data\')\n\nplt.show()\n \n### END File: connection/Basic_metric.ipynb ### \n\n### BEGIN File: note_book/Basic_metric.ipynb ### \nCommit History: \n{"insertions": [959, 837], "deletions": [0, 15], "lines": [959, 852], "committed_datetime": ["2023-12-15 14:37:52", "2023-12-16 21:48:02"], "commit_count": 2} \n Content: \nimport  script\nengine=script.create_conn()\n\nfloat_data=script.fetch_data(engine, "xdr_float_data")\nfloat_data\nfloat_data\n## mean, median, mode is done for the float coloumn\nimport pandas as pd\n\n\n\n\n\n# Exclude \'Bearer Id\', \'IMSI\', \'IMEI\'\n\nselected_columns =float_data.drop([\'Bearer Id\', \'IMSI\', \'IMEI\', \'MSISDN/Number\', \'Start ms\', \'End ms\'], axis=1)\n\n\n\nresults = pd.DataFrame(index=[\'mean\', \'median\', \'mode\', \'variance\', \'std_dev\', \'range\', \'skewness\', \'kurtosis\'])\n\n\n\nfor column in selected_columns:\n\n    mean = selected_columns[column].mean()\n\n    median = selected_columns[column].median()\n\n    mode = selected_columns[column].mode()[0]\n\n    variance = selected_columns[column].var()\n\n    std_dev = selected_columns[column].std()\n\n    data_range = selected_columns[column].max() - selected_columns[column].min()\n\n    skewness = selected_columns[column].skew()\n\n    kurtosis = selected_columns[column].kurtosis()\n\n\n\n    results[column] = [mean, median, mode, variance, std_dev, data_range, skewness, kurtosis]\n\n\n\n# Display the results DataFrame\n\n\n\nresults\nfloat_data.head(10)\nimport matplotlib.pyplot as plt\n\n\n\n# Select columns for analysis\n\ncolumns_for_analysis = ["Dur. (ms)", "Avg RTT DL (ms)", "Avg RTT UL (ms)", "Avg Bearer TP DL (kbps)", "Total UL (Bytes)"]\n\n\n\n# Plot histograms for each selected column\n\nfor column in columns_for_analysis:\n\n    plt.figure(figsize=(8, 5))\n\n    plt.hist(float_data[column].dropna(), bins=50, edgecolor=\'black\', color=\'skyblue\')\n\n    plt.title(f\'Histogram of {column}\')\n\n    plt.xlabel(column)\n\n    plt.ylabel(\'Frequency\')\n\n    plt.show()\n\nimport numpy as np\n\n\n\n\n\ndur_percentile_12 = np.percentile(float_data["Dur. (ms)"].dropna(), 12)\n\nprint(f"The value below which 12% of observations fall is: {dur_percentile_12}")\n\nimport seaborn as sns\n\n\n\n# Select columns for analysis\n\ncolumns_for_analysis = ["Total UL (Bytes)"]\n\n\n\n# Plot box plots for each selected column\n\nfor column in columns_for_analysis:\n\n    plt.figure(figsize=(8, 5))\n\n    sns.boxplot(y=column, data=float_data, color=\'skyblue\')\n\n    plt.title(f\'Box Plot of {column}\')\n\n    plt.show()\n\nimport pandas as pd\n\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n\n\n# Assuming \'float_data\' is your DataFrame\n\n# Select relevant columns for analysis\n\ncolumns_for_analysis = ["Social Media DL (Bytes)", "Social Media UL (Bytes)", "Google DL (Bytes)", "Google UL (Bytes)",\n\n                         "Email DL (Bytes)", "Email UL (Bytes)", "Youtube DL (Bytes)", "Youtube UL (Bytes)",\n\n                         "Netflix DL (Bytes)", "Netflix UL (Bytes)", "Gaming DL (Bytes)", "Gaming UL (Bytes)",\n\n                         "Other DL (Bytes)", "Other UL (Bytes)", "Total DL (Bytes)", "Total UL (Bytes)"]\n\n\n\n# Create a new DataFrame with selected columns\n\nselected_data = float_data[columns_for_analysis]\n\n\n\n# Calculate the correlation matrix\n\ncorrelation_matrix = selected_data.corr()\n\n\n\n# Plot a heatmap for visualization\n\nplt.figure(figsize=(12, 10))\n\nsns.heatmap(correlation_matrix, annot=True, cmap=\'coolwarm\', fmt=".2f", linewidths=.5)\n\nplt.title(\'Correlation Heatmap - Applications vs Total DL+UL Data\')\n\nplt.show()\n \n### END File: note_book/Basic_metric.ipynb ### \n\n### BEGIN File: note_book/Data_cleaing.ipynb ### \nCommit History: \n{"insertions": [1380], "deletions": [0], "lines": [1380], "committed_datetime": ["2023-12-15 14:37:52"], "commit_count": 1} \n Content: \nimport  script\n\nimport pandas as pd\n## importing data from the database \nengine=script.create_conn()\n\ndata=script.fetch_data(engine, "xdr_data")\ndata\n## identifiying the missing fileds in each colomn\nmissing_fields = data.isnull().sum()\n\nprint(missing_fields)\ncolumn_data_types = data.dtypes\n\nprint(column_data_types)\nfloat_columns = data.select_dtypes(include=[\'float64\'])\n\nfloat_columns\nfilled_data = float_columns.fillna(float_columns.mean())\n\nprint(filled_data)\nmissing_fields = filled_data.isnull().sum()\n\nprint(missing_fields)\nimport pandas as pd\n\n\n\n# Create a DataFrame with non-float columns\n\nnon_float_columns = data.select_dtypes(exclude=[\'float64\'])\n\n\n\n\n\ncleand_data = pd.concat([filled_data, non_float_columns], axis=1)\n\n\n\nprint(cleand_data)\n## Damping the cleand data back to the data base\nfrom sqlalchemy import create_engine\n\n\n\n# Create an engine that connects to PostgreSQL server to damp the clean data\n\nengine = create_engine(\'postgresql://postgres:telecom@localhost:5432/telecom\')\n\n\n\n\n\ncleand_data.to_sql(\'xdr_cleaned_data\', engine, if_exists=\'replace\', index=False)\nfrom sqlalchemy import create_engine\n\n\n\n# Create an engine that connects to PostgreSQL server to damp the clean data\n\nengine = create_engine(\'postgresql://postgres:telecom@localhost:5432/telecom\')\n\n\n\n\n\nfloat_columns.to_sql(\'xdr_float_data\', engine, if_exists=\'replace\', index=False) \n### END File: note_book/Data_cleaing.ipynb ### \n\n### BEGIN File: note_book/script.py ### \nCommit History: \n{"insertions": [28], "deletions": [0], "lines": [28], "committed_datetime": ["2023-12-15 14:37:52"], "commit_count": 1} \n Content: \nfrom sqlalchemy import create_engine\nimport pandas as pd\n"""\na function that connect to the local database\n"""\ndef create_conn():\n    engine = None\n    try:\n        # Create an engine that connects to PostgreSQL server\n        engine = create_engine(\'postgresql://postgres:telecom@localhost:5432/telecom\')\n        print("Connection successful")\n    except Exception as error:\n        print(error)\n\n    return engine\n\n"""\na function that that accept engine, and table_name as an argument and return pandas data fream\n"""\ndef fetch_data(engine, table_name):\n    df = None\n    try:\n        # Execute a query and fetch all the rows into a DataFrame\n        df = pd.read_sql_query(f"SELECT * FROM {table_name};", engine)\n    except Exception as error:\n        print(error)\n\n    return df\n \n### END File: note_book/script.py ### \n\n### BEGIN File: note_book/user_behaviour.ipynb ### \nCommit History: \n{"insertions": [641, 7], "deletions": [0, 0], "lines": [641, 7], "committed_datetime": ["2023-12-15 14:37:52", "2023-12-16 21:48:02"], "commit_count": 2} \n Content: \nimport  script\n## fetching the cleaned data from the database\nengine=script.create_conn()\n\ncleaned_data=script.fetch_data(engine, "xdr_cleaned_data")\n# 1. \n\nsessions_per_user = cleaned_data.groupby(\'MSISDN/Number\')[\'Bearer Id\'].nunique()\nsessions_per_user\nimport pandas as pd\n\ncleaned_data[\'Start\'] = pd.to_datetime(cleaned_data[\'Start\'])\n\ncleaned_data[\'End\'] = pd.to_datetime(cleaned_data[\'End\'])\n\ncleaned_data[\'Session Duration\'] = (cleaned_data[\'End\'] - cleaned_data[\'Start\']).dt.total_seconds()\n\ntotal_session_duration = cleaned_data.groupby(\'MSISDN/Number\')[\'Session Duration\'].sum()\ntotal_session_duration\n## total upload and download data per user\nimport pandas as pd\n\n\n\n\n\ntotal_download_per_user = cleaned_data.groupby(\'MSISDN/Number\')[\'Total DL (Bytes)\'].sum()\n\ntotal_upload_per_user = cleaned_data.groupby(\'MSISDN/Number\')[\'Total UL (Bytes)\'].sum()\n\n\n\n# Merge the results into a single DataFrame\n\ntotal_data_per_user = pd.DataFrame({\n\n    \'Total Download (Bytes)\': total_download_per_user,\n\n    \'Total Upload (Bytes)\': total_upload_per_user\n\n}).reset_index()\ntotal_data_per_user\n## total data volume in bytes during the session for each application\ncolumn_headers = cleaned_data.columns\n\ncolumn_headers\nimport pandas as pd\n\n\n\n\n\ndownload_columns = [\'Email DL (Bytes)\', \'Youtube DL (Bytes)\', \'Netflix DL (Bytes)\', \'Gaming DL (Bytes)\', \'Other DL (Bytes)\']\n\nupload_columns = [\'Email UL (Bytes)\', \'Youtube UL (Bytes)\', \'Netflix UL (Bytes)\', \'Gaming UL (Bytes)\', \'Other UL (Bytes)\']\n\n\n\n# Calculate total data volume per application per user\n\ntotal_data_per_user_per_app = cleaned_data.groupby(\'MSISDN/Number\')[download_columns + upload_columns].sum().reset_index()\n\ntotal_data_per_user_per_app  \n### END File: note_book/user_behaviour.ipynb ### \n\n### BEGIN File: note_book/user_engagement.ipynb ### \nCommit History: \n{"insertions": [438, 4], "deletions": [424, 4], "lines": [862, 8], "committed_datetime": ["2023-12-15 23:04:05", "2023-12-16 21:48:02"], "commit_count": 2} \n Content: \nimport  script\nengine=script.create_conn()\n\ncleaned_data=script.fetch_data(engine, "xdr_cleaned_data")\ncleaned_data.columns\n\nimport pandas as pd\n\nfrom sklearn.cluster import KMeans\n\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\n\n\n\n\n\n# Extract relevant columns for engagement metrics\n\nengagement_metrics = cleaned_data[[\'MSISDN/Number\', \'Dur. (ms)\', \'Total UL (Bytes)\', \'Total DL (Bytes)\']]\n\n\n\n#  Aggregate the metrics per customer ID and report the top 10 customers\n\nagg_engagement = engagement_metrics.groupby(\'MSISDN/Number\').agg({\n\n    \'Dur. (ms)\': \'sum\',\n\n    \'Total UL (Bytes)\': \'sum\',\n\n    \'Total DL (Bytes)\': \'sum\'\n\n}).reset_index()\n\n\n\n\n\nagg_engagement[\'Total Engagement\'] = agg_engagement[\'Dur. (ms)\'] + agg_engagement[\'Total UL (Bytes)\'] + agg_engagement[\'Total DL (Bytes)\']\n\n\n\n\n\ntop_10_customers = agg_engagement.sort_values(\'Total Engagement\', ascending=False).head(10)\n\n\n\ntop_10_customers\nimport pandas as pd\n\nfrom sklearn.cluster import KMeans\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n\n\n\ncluster_data = agg_engagement[[\'Dur. (ms)\', \'Total UL (Bytes)\', \'Total DL (Bytes)\']]\n\n\n\n# Min-Max scaling\n\nscaler = MinMaxScaler()\n\nnormalized_data = scaler.fit_transform(cluster_data)\n\n\n\n\n\n\n\n\n\n# Run K-Means clustering (k=3)\n\nkmeans = KMeans(n_clusters=3, random_state=42)\n\nagg_engagement[\'Cluster\'] = kmeans.fit_predict(normalized_data)\n\n\n\n\n\ncluster_summary = agg_engagement.groupby(\'Cluster\').agg({\n\n    \'Dur. (ms)\': [\'min\', \'max\', \'mean\', \'sum\'],\n\n    \'Total UL (Bytes)\': [\'min\', \'max\', \'mean\', \'sum\'],\n\n    \'Total DL (Bytes)\': [\'min\', \'max\', \'mean\', \'sum\']\n\n}).reset_index()\n\n\n\n\n\n\n\ncluster_summary\nimport pandas as pd\n\n\n\n\n\n\n\n# Step 1: Create a new column for total traffic per application\n\ncleaned_data[\'Social Media Traffic\'] = cleaned_data[\'Social Media DL (Bytes)\'] + cleaned_data[\'Social Media UL (Bytes)\']\n\ncleaned_data[\'Google Traffic\'] = cleaned_data[\'Google DL (Bytes)\'] + cleaned_data[\'Google UL (Bytes)\']\n\ncleaned_data[\'Email Traffic\'] = cleaned_data[\'Email DL (Bytes)\'] + cleaned_data[\'Email UL (Bytes)\']\n\ncleaned_data[\'Youtube Traffic\'] = cleaned_data[\'Youtube DL (Bytes)\'] + cleaned_data[\'Youtube UL (Bytes)\']\n\ncleaned_data[\'Netflix Traffic\'] = cleaned_data[\'Netflix DL (Bytes)\'] + cleaned_data[\'Netflix UL (Bytes)\']\n\ncleaned_data[\'Gaming Traffic\'] = cleaned_data[\'Gaming DL (Bytes)\'] + cleaned_data[\'Gaming UL (Bytes)\']\n\ncleaned_data[\'Other Traffic\'] = cleaned_data[\'Other DL (Bytes)\'] + cleaned_data[\'Other UL (Bytes)\']\n\n\n\n#  Group by \'MSISDN/Number\' and find the sum of total traffic for each application\n\nagg_user_app_traffic = cleaned_data.groupby(\'MSISDN/Number\')[[\'Social Media Traffic\', \'Google Traffic\', \'Email Traffic\',\n\n                                                   \'Youtube Traffic\', \'Netflix Traffic\', \'Gaming Traffic\', \'Other Traffic\']].sum().reset_index()\n\n\n\n#  Find the top 10 most engaged users per application\n\ntop_10_social_media_users = agg_user_app_traffic.nlargest(10, \'Social Media Traffic\')\n\ntop_10_google_users = agg_user_app_traffic.nlargest(10, \'Google Traffic\')\n\ntop_10_email_users = agg_user_app_traffic.nlargest(10, \'Email Traffic\')\n\ntop_10_youtube_users = agg_user_app_traffic.nlargest(10, \'Youtube Traffic\')\n\ntop_10_netflix_users = agg_user_app_traffic.nlargest(10, \'Netflix Traffic\')\n\ntop_10_gaming_users = agg_user_app_traffic.nlargest(10, \'Gaming Traffic\')\n\ntop_10_other_users = agg_user_app_traffic.nlargest(10, \'Other Traffic\')\n\n\n\n\n\ntop_10_social_media_users\n\n \n# Find the top 3 most used applications\n\ntop_3_apps = agg_user_app_traffic[[\'Social Media Traffic\', \'Google Traffic\', \'Email Traffic\',\n\n                                   \'Youtube Traffic\', \'Netflix Traffic\', \'Gaming Traffic\', \'Other Traffic\']].sum().nlargest(3)\n\n\n\n# Plot the bar chart\n\ntop_3_apps.plot(kind=\'bar\', rot=0, color=\'skyblue\')\n\nplt.title(\'Top 3 Most Used Applications\')\n\nplt.xlabel(\'Application\')\n\nplt.ylabel(\'Total Traffic\')\n\nplt.show() \n### END File: note_book/user_engagement.ipynb ### \n\n### BEGIN File: app/main.py ### \nCommit History: \n{"insertions": [19], "deletions": [0], "lines": [19], "committed_datetime": ["2023-12-16 16:32:15"], "commit_count": 1} \n Content: \nimport streamlit as st\nfrom over_view import over_view\nfrom user_engagment import engagement\nfrom experience_analytics import experiencd\nfrom satisfaction_analytics import satisfaction\n\n# Sidebar for navigation\npage_options = ["Over view", "User Engagment", "Experience Analytics", "Satisfaction Analytics"]\nselected_page = st.sidebar.selectbox("Select a page", page_options)\n\n# Display the selected page\nif selected_page == "Over view":\n    over_view()\nelif selected_page == "User Engagment":\n    engagement()\nelif selected_page == "Experience Analytics":\n    experiencd()\nelif selected_page == "Satisfaction Analytics":\n    satisfaction()\n \n### END File: app/main.py ### \n\n### BEGIN File: app/other_module.py ### \nCommit History: \n{"insertions": [28], "deletions": [0], "lines": [28], "committed_datetime": ["2023-12-16 17:36:36"], "commit_count": 1} \n Content: \nfrom sqlalchemy import create_engine\nimport pandas as pd\n"""\na function that connect to the local database\n"""\ndef create_conn():\n    engine = None\n    try:\n        # Create an engine that connects to PostgreSQL server\n        engine = create_engine(\'postgresql://postgres:telecom@localhost:5432/telecom\')\n        print("Connection successful")\n    except Exception as error:\n        print(error)\n\n    return engine\n\n"""\na function that that accept engine, and table_name as an argument and return pandas data fream\n"""\ndef fetch_data(engine, table_name):\n    df = None\n    try:\n        # Execute a query and fetch all the rows into a DataFrame\n        df = pd.read_sql_query(f"SELECT * FROM {table_name};", engine)\n    except Exception as error:\n        print(error)\n\n    return df\n \n### END File: app/other_module.py ### \n\n### BEGIN File: app/over_view.py ### \nCommit History: \n{"insertions": [15, 3], "deletions": [1, 1], "lines": [16, 4], "committed_datetime": ["2023-12-16 17:36:36", "2023-12-16 21:43:01"], "commit_count": 2} \n Content: \nimport streamlit as st\nfrom other_module import fetch_data, create_conn\nfrom visualization import create_top_10_handsets_chart, create_top_3_manufacturers_chart, create_top_5_handsets_per_manufacturer_chart\n\ndef over_view():\n    st.title("Over view")\n    st.write("This is Over view analysis TellCo\'s company")\n\n    engine = create_conn()\n    data = fetch_data(engine, "xdr_data")\n    cleaned_data = fetch_data(engine, "xdr_cleaned_data")\n    \n    \n    st.write("#Top 10 Handsets")\n    create_top_10_handsets_chart(data)\n    st.write("#Top 3 Manufacturers")\n    create_top_3_manufacturers_chart(data)\n\n    st.write("#top_5_handsets_per_manufacturer")\n    create_top_5_handsets_per_manufacturer_chart(data)\n\n \n### END File: app/over_view.py ### \n\n### BEGIN File: app/visualization.py ### \nCommit History: \n{"insertions": [16], "deletions": [11], "lines": [27], "committed_datetime": ["2023-12-16 21:43:01"], "commit_count": 1} \n Content: \nimport streamlit as st\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom other_module import fetch_data, create_conn\n"""\n    function that plot top 10 handsets type and it\'s count\n"""\ndef create_top_10_handsets_chart(data):\n    top_10_handsets = data[\'Handset Type\'].value_counts().head(10)\n    fig = px.bar(top_10_handsets, x=top_10_handsets.index, y=top_10_handsets.values, labels={\'x\': \'Handset Type\', \'y\': \'Count\'})\n    fig.update_layout(title=\'Top 10 Handsets\')\n    st.plotly_chart(fig)\n"""\n    function that plot top 3 manufacturers with it\'s count \n"""\ndef create_top_3_manufacturers_chart(data):\n    top_3_manufactures = data[\'Handset Manufacturer\'].value_counts().head(3)\n    fig = px.bar(top_3_manufactures, x=top_3_manufactures.index, y=top_3_manufactures.values, labels={\'x\': \'Manufacturer\', \'y\': \'Count\'})\n    fig.update_layout(title=\'Top 3 Handset Manufacturers\')\n    st.plotly_chart(fig)\n"""\n    function that plot top 5 handsets per manufacturer with it\'s type\n"""\ndef create_top_5_handsets_per_manufacturer_chart(data):\n    top_3_manufacturers = data[\'Handset Manufacturer\'].value_counts().head(3).index\n    filtered_data = data[data[\'Handset Manufacturer\'].isin(top_3_manufacturers)]\n\n    top_5_handsets_per_manufacturer = filtered_data.groupby([\'Handset Manufacturer\', \'Handset Type\']).size().reset_index(name=\'Count\')\n\n    fig = go.Figure(data=go.Heatmap(\n        z=top_5_handsets_per_manufacturer[\'Count\'],\n        x=top_5_handsets_per_manufacturer[\'Handset Manufacturer\'],\n        y=top_5_handsets_per_manufacturer[\'Handset Type\'],\n        colorscale=\'Viridis\',\n        colorbar=dict(title=\'Count\'),\n    ))\n\n    fig.update_layout(title=\'Top 5 Handsets per Manufacturer\',\n                      xaxis=dict(title=\'Manufacturer\'),\n                      yaxis=dict(title=\'Handset Type\'))\n\n    st.plotly_chart(fig)\n\n\n\n\n \n### END File: app/visualization.py ### \n\n### BEGIN File: note_book/User Overview analysis.ipynb ### \nCommit History: \n{"insertions": [61], "deletions": [396], "lines": [457], "committed_datetime": ["2023-12-16 21:48:02"], "commit_count": 1} \n Content: \nimport  script\n## importing data from the database \nengine=script.create_conn()\n\ndata=script.fetch_data(engine, "xdr_data")\ndata.dtypes\ncolumn_names_list = data.columns.tolist()\n\nprint(column_names_list)\n## prining the first  20 handset type\nprint(data[\'Handset Type\'].head(20))\n## pringing the top 10 handset type\ntop_10_handsets = data[\'Handset Type\'].value_counts().head(10)\n\nprint(top_10_handsets)\n## prining the top 3 manufactures\ntop_3_manufactures = data[\'Handset Manufacturer\'].value_counts().head(3)\n\nprint(top_3_manufactures)\n## prining the top 5 handset per manfacturer \n\n\ntop_3_manufacturers = data[\'Handset Manufacturer\'].value_counts().head(3).index\n\nfiltered_data = data[data[\'Handset Manufacturer\'].isin(top_3_manufacturers)]\n\n\n\n\n\ntop_5_handsets_per_manufacturer = filtered_data.groupby(\'Handset Manufacturer\')[\'Handset Type\'].value_counts().groupby(level=0, group_keys=False).nlargest(5)\n\nprint(top_5_handsets_per_manufacturer)\n \n### END File: note_book/User Overview analysis.ipynb ### \n'
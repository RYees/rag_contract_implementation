{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U \"langchain==0.0.344\" openai tiktoken lark datasets sentence_transformers FlagEmbedding lancedb -qq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import LanceDB\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "\n",
    "#Text Splitting\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "import lancedb\n",
    "from dotenv import dotenv_values\n",
    "import openai\n",
    "\n",
    "env_vars = dotenv_values('.env')\n",
    "openai.api_key = \"sk-proj-Tzc9mrWyEFVxyDsq5HiWT3BlbkFJxp47toOztG4XRILBeRxr\"\n",
    "\n",
    "# Embedding Functions\n",
    "# model_name = \"BAAI/bge-small-en-v1.5\" # Open Source and effective Embedding\n",
    "# encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "# bge_embeddings = HuggingFaceBgeEmbeddings(model_name=model_name,model_kwargs={'device': 'cuda'},encode_kwargs=encode_kwargs)\n",
    "bge_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=openai.api_key)\n",
    "# Data Chunking Functions\n",
    "small_chunk_splitter = RecursiveCharacterTextSplitter(chunk_size = 512) # Splitter to split documents into small chunks\n",
    "big_chunk_splitter = RecursiveCharacterTextSplitter(chunk_size=2048) # Another Level of Bigger Chunks\n",
    "\n",
    "# Lance DB Connection. Load if exists else create\n",
    "my_db = lancedb.connect(\"./my_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-Tzc9mrWyEFVxyDsq5HiWT3BlbkFJxp47toOztG4XRILBeRxr\n"
     ]
    }
   ],
   "source": [
    "print(openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample data here\n",
    "long_texts = load_dataset(\"huggingartists/eminem\")[\"train\"].to_pandas().sample(100)[\"text\"] # Data of huge context length. Use 100 random examples for demo\n",
    "\n",
    "# Convert to LangChain Document object\n",
    "docs = [Document(page_content=content, doc_id = _id, metadata = {\"doc_id\":_id}) for (_id, content) in enumerate(long_texts)] # List of LangChain Document Objects\n",
    "\n",
    "\n",
    "# if \"small_chunk_table\" in my_db.table_names():\n",
    "#   small_chunk_table = my_db.open_table(\"small_chunk_table\")\n",
    "# else: # NOTE: 384 is the size of BAAI Embedding and -999 because it's a dummy data so invalid Embedding\n",
    "small_chunk_table = my_db.create_table(\"small_chunk_table\", data=[{\"vector\": [], \"text\": \"\", \"doc_id\": \"-1\",}], mode=\"overwrite\")\n",
    "\n",
    "# small_chunk_table.delete('doc_id = \"-1\"')\n",
    "\n",
    "vectorstore = LanceDB(small_chunk_table, bge_embeddings) # Vectorstore to use to index the child chunks\n",
    "store = InMemoryStore() # The storage layer for the parent documents\n",
    "\n",
    "full_doc_retriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=small_chunk_splitter)\n",
    "\n",
    "full_doc_retriever.add_documents(docs, ids=None) # Add all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bge_embeddings.embed_query(\"test\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch 3 most similar Smaller Documents\n",
    "sub_docs = vectorstore.similarity_search(\"I am whatever you say I am and if I wasn't why would you say I am\", k=3)\n",
    "\n",
    "print(sub_docs[0].page_content) # This is a Smaller Chunk.\n",
    "\n",
    "\n",
    "full_docs = full_doc_retriever.get_relevant_documents(\"I am whatever you say I am and if I wasn't why would you say I am\", k = 3)\n",
    "print(full_docs[0].page_content) # This is the Parent Document returned after matching the smaller chunks internally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigger Chunk Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"big_chunk_table\" in my_db.table_names():\n",
    "  big_chunk_table = my_db.open_table(\"big_chunk_table\")\n",
    "else:\n",
    "  big_chunk_table = my_db.create_table(\"big_chunk_table\", data=[{\"vector\": [-999]*384, \"text\": \"\", \"doc_id\": \"-1\",}], mode=\"overwrite\")\n",
    "\n",
    "big_chunk_table.delete('doc_id = \"-1\"')\n",
    "\n",
    "vectorstore = LanceDB(big_chunk_table, bge_embeddings)\n",
    "store = InMemoryStore()\n",
    "\n",
    "big_chunk_retriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store,\n",
    "                                              child_splitter = small_chunk_splitter,\n",
    "                                              parent_splitter = big_chunk_splitter) # See one more line addition which retrieves the larger chunk instead of Parent Document\n",
    "\n",
    "big_chunk_retriever.add_documents(docs, ids=None) # Add all the documents\n",
    "\n",
    "\n",
    "big_chunks_docs = big_chunk_retriever.get_relevant_documents(\"I am whatever you say I am and if I wasn't why would you say I am\", k = 3)\n",
    "print(big_chunks_docs[0].page_content) # This is the BIG chunks (in place of Parent Document) returned after matching the smaller chunks internally"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

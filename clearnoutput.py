'   top-3 socall medai plat forms.png .gitignore README.md app    main.py    over_view.py    satisfaction_analytics.py    other_module.py    experience_analytics.py    __pycache__    user_engagment.py    visualization.py note_book      __init__.py     Basic_metric.ipynb     Data_cleaing.ipynb     user_behaviour.ipynb     script.py     __pycache__     user_engagement.ipynb     User Overview analysis.ipynb   BEGIN File: README.md  Commit History: {insertions: [2], deletions: [0], lines: [2], committed_datetime: [ ], commit_count: 1}  Content: # Week-1-User-Analytics-in-the-Telecommunication-Industry  END File: README.md   BEGIN File: connection/script.py  Commit History: {insertions: [1, 2], deletions: [1, 2], lines: [2, 4], committed_datetime: [ ,  ], commit_count: 2}  Content: from sqlalchemy import create_engineimport pandas as pddef create_conn():    engine = None    try:        # Create an engine that connects to PostgreSQL server        engine = create_engine(postgresql://postgres:telecom@localhost:5432/telecom)        print(Connection successful)    except Exception as error:        print(error)    return enginedef fetch_data(engine, table_name):    df = None    try:        # Execute a query and fetch all the rows into a DataFrame        df = pd.read_sql_query(fSELECT * FROM {table_name};, engine)    except Exception as error:        print(error)    return df  END File: connection/script.py   BEGIN File: connection/Data_cleaing.ipynb  Commit History: {insertions: [762, 542, 687, 26, 36], deletions: [2, 0, 661, 0, 10], lines: [764, 542, 1348, 26, 46], committed_datetime: [ ,  ,  ,  ,  ], commit_count: 5}  Content: import  scriptimport pandas as pd## importing data from the database engine=script.create_conn()data=script.fetch_data(engine, xdr_data)data## identifiying the missing fileds in each colomnmissing_fields = data.isnull().sum()print(missing_fields)column_data_types = data.dtypesprint(column_data_types)float_columns = data.select_dtypes(include=[float64])float_columnsfilled_data = float_columns.fillna(float_columns.mean())print(filled_data)missing_fields = filled_data.isnull().sum()print(missing_fields)import pandas as pd# Create a DataFrame with non-float columnsnon_float_columns = data.select_dtypes(exclude=[float64])cleand_data = pd.concat([filled_data, non_float_columns], axis=1)print(cleand_data)## Damping the cleand data back to the data basefrom sqlalchemy import create_engine# Create an engine that connects to PostgreSQL server to damp the clean dataengine = create_engine(postgresql://postgres:telecom@localhost:5432/telecom)cleand_data.to_sql(xdr_cleaned_data, engine, if_exists=replace, index=False)from sqlalchemy import create_engine# Create an engine that connects to PostgreSQL server to damp the clean dataengine = create_engine(postgresql://postgres:telecom@localhost:5432/telecom)float_columns.to_sql(xdr_float_data, engine, if_exists=replace, index=False)  END File: connection/Data_cleaing.ipynb   BEGIN File: connection/User Overview analysis.ipynb  Commit History: {insertions: [38], deletions: [8], lines: [46], committed_datetime: [ ], commit_count: 1}  Content: import  script## importing data from the database engine=script.create_conn()data=script.fetch_data(engine, xdr_data)datacolumn_names_list = data.columns.tolist()print(column_names_list)## prining the first  20 handset typeprint(data[Handset Type].head(20))## pringing the top 10 handset typetop_10_handsets = data[Handset Type].value_counts().head(10)print(top_10_handsets)## prining the top 3 manufacturestop_3_manufactures = data[Handset Manufacturer].value_counts().head(3)print(top_3_manufactures)## prining the top 5 handset per manfacturer top_3_manufacturers = data[Handset Manufacturer].value_counts().head(3).indexfiltered_data = data[data[Handset Manufacturer].isin(top_3_manufacturers)]top_5_handsets_per_manufacturer = filtered_data.groupby(Handset Manufacturer)[Handset Type].value_counts().groupby(level=0, group_keys=False).nlargest(5)print(top_5_handsets_per_manufacturer)  END File: connection/User Overview analysis.ipynb   BEGIN File: connection/user_behaviour.ipynb  Commit History: {insertions: [503], deletions: [1], lines: [504], committed_datetime: [ ], commit_count: 1}  Content: import  script## fetching the cleaned data from the databaseengine=script.create_conn()cleaned_data=script.fetch_data(engine, xdr_cleaned_data)# 1. sessions_per_user = cleaned_data.groupby(MSISDN/Number)[Bearer Id].nunique()sessions_per_userimport pandas as pdcleaned_data[Start] = pd.to_datetime(cleaned_data[Start])cleaned_data[End] = pd.to_datetime(cleaned_data[End])cleaned_data[Session Duration] = (cleaned_data[End] - cleaned_data[Start]).dt.total_seconds()total_session_duration = cleaned_data.groupby(MSISDN/Number)[Session Duration].sum()total_session_duration## total upload and download data per userimport pandas as pdtotal_download_per_user = cleaned_data.groupby(MSISDN/Number)[Total DL (Bytes)].sum()total_upload_per_user = cleaned_data.groupby(MSISDN/Number)[Total UL (Bytes)].sum()# Merge the results into a single DataFrametotal_data_per_user = pd.DataFrame({    Total Download (Bytes): total_download_per_user,    Total Upload (Bytes): total_upload_per_user}).reset_index()total_data_per_user## total data volume in bytes during the session for each applicationcolumn_headers = cleaned_data.columnscolumn_headersimport pandas as pddownload_columns = [Email DL (Bytes), Youtube DL (Bytes), Netflix DL (Bytes), Gaming DL (Bytes), Other DL (Bytes)]upload_columns = [Email UL (Bytes), Youtube UL (Bytes), Netflix UL (Bytes), Gaming UL (Bytes), Other UL (Bytes)]# Calculate total data volume per application per usertotal_data_per_user_per_app = cleaned_data.groupby(MSISDN/Number)[download_columns + upload_columns].sum().reset_index()total_data_per_user_per_app   END File: connection/user_behaviour.ipynb   BEGIN File: connection/Basic_metric.ipynb  Commit History: {insertions: [731], deletions: [54], lines: [785], committed_datetime: [ ], commit_count: 1}  Content: import  scriptengine=script.create_conn()float_data=script.fetch_data(engine, xdr_float_data)float_data## mean, median, mode is done for the float coloumnimport pandas as pd# Exclude Bearer Id, IMSI, IMEIselected_columns =float_data.drop([Bearer Id, IMSI, IMEI, MSISDN/Number, Start ms, End ms], axis=1)results = pd.DataFrame(index=[mean, median, mode, variance, std_dev, range, skewness, kurtosis])for column in selected_columns:    mean = selected_columns[column].mean()    median = selected_columns[column].median()    mode = selected_columns[column].mode()[0]    variance = selected_columns[column].var()    std_dev = selected_columns[column].std()    data_range = selected_columns[column].max() - selected_columns[column].min()    skewness = selected_columns[column].skew()    kurtosis = selected_columns[column].kurtosis()    results[column] = [mean, median, mode, variance, std_dev, data_range, skewness, kurtosis]# Display the results DataFrameresultsfloat_data.head(10)import matplotlib.pyplot as plt# Select columns for analysiscolumns_for_analysis = [Dur. (ms), Avg RTT DL (ms), Avg RTT UL (ms), Avg Bearer TP DL (kbps), Total UL (Bytes)]# Plot histograms for each selected columnfor column in columns_for_analysis:    plt.figure(figsize=(8, 5))    plt.hist(float_data[column].dropna(), bins=30, edgecolor=black, color=skyblue)    plt.title(fHistogram of {column})    plt.xlabel(column)    plt.ylabel(Frequency)    plt.show()import seaborn as sns# Select columns for analysiscolumns_for_analysis = [Total UL (Bytes)]# Plot box plots for each selected columnfor column in columns_for_analysis:    plt.figure(figsize=(8, 5))    sns.boxplot(y=column, data=float_data, color=skyblue)    plt.title(fBox Plot of {column})    plt.show()import pandas as pdimport seaborn as snsimport matplotlib.pyplot as plt# Assuming float_data is your DataFrame# Select relevant columns for analysiscolumns_for_analysis = [Social Media DL (Bytes), Social Media UL (Bytes), Google DL (Bytes), Google UL (Bytes),                         Email DL (Bytes), Email UL (Bytes), Youtube DL (Bytes), Youtube UL (Bytes),                         Netflix DL (Bytes), Netflix UL (Bytes), Gaming DL (Bytes), Gaming UL (Bytes),                         Other DL (Bytes), Other UL (Bytes), Total DL (Bytes), Total UL (Bytes)]# Create a new DataFrame with selected columnsselected_data = float_data[columns_for_analysis]# Calculate the correlation matrixcorrelation_matrix = selected_data.corr()# Plot a heatmap for visualizationplt.figure(figsize=(12, 10))sns.heatmap(correlation_matrix, annot=True, cmap=coolwarm, fmt=.2f, linewidths=.5)plt.title(Correlation Heatmap - Applications vs Total DL+UL Data)plt.show()  END File: connection/Basic_metric.ipynb   BEGIN File: note_book/Basic_metric.ipynb  Commit History: {insertions: [959, 837], deletions: [0, 15], lines: [959, 852], committed_datetime: [ ,  ], commit_count: 2}  Content: import  scriptengine=script.create_conn()float_data=script.fetch_data(engine, xdr_float_data)float_datafloat_data## mean, median, mode is done for the float coloumnimport pandas as pd# Exclude Bearer Id, IMSI, IMEIselected_columns =float_data.drop([Bearer Id, IMSI, IMEI, MSISDN/Number, Start ms, End ms], axis=1)results = pd.DataFrame(index=[mean, median, mode, variance, std_dev, range, skewness, kurtosis])for column in selected_columns:    mean = selected_columns[column].mean()    median = selected_columns[column].median()    mode = selected_columns[column].mode()[0]    variance = selected_columns[column].var()    std_dev = selected_columns[column].std()    data_range = selected_columns[column].max() - selected_columns[column].min()    skewness = selected_columns[column].skew()    kurtosis = selected_columns[column].kurtosis()    results[column] = [mean, median, mode, variance, std_dev, data_range, skewness, kurtosis]# Display the results DataFrameresultsfloat_data.head(10)import matplotlib.pyplot as plt# Select columns for analysiscolumns_for_analysis = [Dur. (ms), Avg RTT DL (ms), Avg RTT UL (ms), Avg Bearer TP DL (kbps), Total UL (Bytes)]# Plot histograms for each selected columnfor column in columns_for_analysis:    plt.figure(figsize=(8, 5))    plt.hist(float_data[column].dropna(), bins=50, edgecolor=black, color=skyblue)    plt.title(fHistogram of {column})    plt.xlabel(column)    plt.ylabel(Frequency)    plt.show()import numpy as npdur_percentile_12 = np.percentile(float_data[Dur. (ms)].dropna(), 12)print(fThe value below which 12% of observations fall is: {dur_percentile_12})import seaborn as sns# Select columns for analysiscolumns_for_analysis = [Total UL (Bytes)]# Plot box plots for each selected columnfor column in columns_for_analysis:    plt.figure(figsize=(8, 5))    sns.boxplot(y=column, data=float_data, color=skyblue)    plt.title(fBox Plot of {column})    plt.show()import pandas as pdimport seaborn as snsimport matplotlib.pyplot as plt# Assuming float_data is your DataFrame# Select relevant columns for analysiscolumns_for_analysis = [Social Media DL (Bytes), Social Media UL (Bytes), Google DL (Bytes), Google UL (Bytes),                         Email DL (Bytes), Email UL (Bytes), Youtube DL (Bytes), Youtube UL (Bytes),                         Netflix DL (Bytes), Netflix UL (Bytes), Gaming DL (Bytes), Gaming UL (Bytes),                         Other DL (Bytes), Other UL (Bytes), Total DL (Bytes), Total UL (Bytes)]# Create a new DataFrame with selected columnsselected_data = float_data[columns_for_analysis]# Calculate the correlation matrixcorrelation_matrix = selected_data.corr()# Plot a heatmap for visualizationplt.figure(figsize=(12, 10))sns.heatmap(correlation_matrix, annot=True, cmap=coolwarm, fmt=.2f, linewidths=.5)plt.title(Correlation Heatmap - Applications vs Total DL+UL Data)plt.show()  END File: note_book/Basic_metric.ipynb   BEGIN File: note_book/Data_cleaing.ipynb  Commit History: {insertions: [1380], deletions: [0], lines: [1380], committed_datetime: [ ], commit_count: 1}  Content: import  scriptimport pandas as pd## importing data from the database engine=script.create_conn()data=script.fetch_data(engine, xdr_data)data## identifiying the missing fileds in each colomnmissing_fields = data.isnull().sum()print(missing_fields)column_data_types = data.dtypesprint(column_data_types)float_columns = data.select_dtypes(include=[float64])float_columnsfilled_data = float_columns.fillna(float_columns.mean())print(filled_data)missing_fields = filled_data.isnull().sum()print(missing_fields)import pandas as pd# Create a DataFrame with non-float columnsnon_float_columns = data.select_dtypes(exclude=[float64])cleand_data = pd.concat([filled_data, non_float_columns], axis=1)print(cleand_data)## Damping the cleand data back to the data basefrom sqlalchemy import create_engine# Create an engine that connects to PostgreSQL server to damp the clean dataengine = create_engine(postgresql://postgres:telecom@localhost:5432/telecom)cleand_data.to_sql(xdr_cleaned_data, engine, if_exists=replace, index=False)from sqlalchemy import create_engine# Create an engine that connects to PostgreSQL server to damp the clean dataengine = create_engine(postgresql://postgres:telecom@localhost:5432/telecom)float_columns.to_sql(xdr_float_data, engine, if_exists=replace, index=False)  END File: note_book/Data_cleaing.ipynb   BEGIN File: note_book/script.py  Commit History: {insertions: [28], deletions: [0], lines: [28], committed_datetime: [ ], commit_count: 1}  Content: from sqlalchemy import create_engineimport pandas as pda function that connect to the local databasedef create_conn():    engine = None    try:        # Create an engine that connects to PostgreSQL server        engine = create_engine(postgresql://postgres:telecom@localhost:5432/telecom)        print(Connection successful)    except Exception as error:        print(error)    return enginea function that that accept engine, and table_name as an argument and return pandas data freamdef fetch_data(engine, table_name):    df = None    try:        # Execute a query and fetch all the rows into a DataFrame        df = pd.read_sql_query(fSELECT * FROM {table_name};, engine)    except Exception as error:        print(error)    return df  END File: note_book/script.py   BEGIN File: note_book/user_behaviour.ipynb  Commit History: {insertions: [641, 7], deletions: [0, 0], lines: [641, 7], committed_datetime: [ ,  ], commit_count: 2}  Content: import  script## fetching the cleaned data from the databaseengine=script.create_conn()cleaned_data=script.fetch_data(engine, xdr_cleaned_data)# 1. sessions_per_user = cleaned_data.groupby(MSISDN/Number)[Bearer Id].nunique()sessions_per_userimport pandas as pdcleaned_data[Start] = pd.to_datetime(cleaned_data[Start])cleaned_data[End] = pd.to_datetime(cleaned_data[End])cleaned_data[Session Duration] = (cleaned_data[End] - cleaned_data[Start]).dt.total_seconds()total_session_duration = cleaned_data.groupby(MSISDN/Number)[Session Duration].sum()total_session_duration## total upload and download data per userimport pandas as pdtotal_download_per_user = cleaned_data.groupby(MSISDN/Number)[Total DL (Bytes)].sum()total_upload_per_user = cleaned_data.groupby(MSISDN/Number)[Total UL (Bytes)].sum()# Merge the results into a single DataFrametotal_data_per_user = pd.DataFrame({    Total Download (Bytes): total_download_per_user,    Total Upload (Bytes): total_upload_per_user}).reset_index()total_data_per_user## total data volume in bytes during the session for each applicationcolumn_headers = cleaned_data.columnscolumn_headersimport pandas as pddownload_columns = [Email DL (Bytes), Youtube DL (Bytes), Netflix DL (Bytes), Gaming DL (Bytes), Other DL (Bytes)]upload_columns = [Email UL (Bytes), Youtube UL (Bytes), Netflix UL (Bytes), Gaming UL (Bytes), Other UL (Bytes)]# Calculate total data volume per application per usertotal_data_per_user_per_app = cleaned_data.groupby(MSISDN/Number)[download_columns + upload_columns].sum().reset_index()total_data_per_user_per_app   END File: note_book/user_behaviour.ipynb   BEGIN File: note_book/user_engagement.ipynb  Commit History: {insertions: [438, 4], deletions: [424, 4], lines: [862, 8], committed_datetime: [ ,  ], commit_count: 2}  Content: import  scriptengine=script.create_conn()cleaned_data=script.fetch_data(engine, xdr_cleaned_data)cleaned_data.columnsimport pandas as pdfrom sklearn.cluster import KMeansfrom sklearn.preprocessing import StandardScalerimport matplotlib.pyplot as plt# Extract relevant columns for engagement metricsengagement_metrics = cleaned_data[[MSISDN/Number, Dur. (ms), Total UL (Bytes), Total DL (Bytes)]]#  Aggregate the metrics per customer ID and report the top 10 customersagg_engagement = engagement_metrics.groupby(MSISDN/Number).agg({    Dur. (ms): sum,    Total UL (Bytes): sum,    Total DL (Bytes): sum}).reset_index()agg_engagement[Total Engagement] = agg_engagement[Dur. (ms)] + agg_engagement[Total UL (Bytes)] + agg_engagement[Total DL (Bytes)]top_10_customers = agg_engagement.sort_values(Total Engagement, ascending=False).head(10)top_10_customersimport pandas as pdfrom sklearn.cluster import KMeansfrom sklearn.preprocessing import MinMaxScalercluster_data = agg_engagement[[Dur. (ms), Total UL (Bytes), Total DL (Bytes)]]# Min-Max scalingscaler = MinMaxScaler()normalized_data = scaler.fit_transform(cluster_data)# Run K-Means clustering (k=3)kmeans = KMeans(n_clusters=3, random_state=42)agg_engagement[Cluster] = kmeans.fit_predict(normalized_data)cluster_summary = agg_engagement.groupby(Cluster).agg({    Dur. (ms): [min, max, mean, sum],    Total UL (Bytes): [min, max, mean, sum],    Total DL (Bytes): [min, max, mean, sum]}).reset_index()cluster_summaryimport pandas as pd# Step 1: Create a new column for total traffic per applicationcleaned_data[Social Media Traffic] = cleaned_data[Social Media DL (Bytes)] + cleaned_data[Social Media UL (Bytes)]cleaned_data[Google Traffic] = cleaned_data[Google DL (Bytes)] + cleaned_data[Google UL (Bytes)]cleaned_data[Email Traffic] = cleaned_data[Email DL (Bytes)] + cleaned_data[Email UL (Bytes)]cleaned_data[Youtube Traffic] = cleaned_data[Youtube DL (Bytes)] + cleaned_data[Youtube UL (Bytes)]cleaned_data[Netflix Traffic] = cleaned_data[Netflix DL (Bytes)] + cleaned_data[Netflix UL (Bytes)]cleaned_data[Gaming Traffic] = cleaned_data[Gaming DL (Bytes)] + cleaned_data[Gaming UL (Bytes)]cleaned_data[Other Traffic] = cleaned_data[Other DL (Bytes)] + cleaned_data[Other UL (Bytes)]#  Group by MSISDN/Number and find the sum of total traffic for each applicationagg_user_app_traffic = cleaned_data.groupby(MSISDN/Number)[[Social Media Traffic, Google Traffic, Email Traffic,                                                   Youtube Traffic, Netflix Traffic, Gaming Traffic, Other Traffic]].sum().reset_index()#  Find the top 10 most engaged users per applicationtop_10_social_media_users = agg_user_app_traffic.nlargest(10, Social Media Traffic)top_10_google_users = agg_user_app_traffic.nlargest(10, Google Traffic)top_10_email_users = agg_user_app_traffic.nlargest(10, Email Traffic)top_10_youtube_users = agg_user_app_traffic.nlargest(10, Youtube Traffic)top_10_netflix_users = agg_user_app_traffic.nlargest(10, Netflix Traffic)top_10_gaming_users = agg_user_app_traffic.nlargest(10, Gaming Traffic)top_10_other_users = agg_user_app_traffic.nlargest(10, Other Traffic)top_10_social_media_users # Find the top 3 most used applicationstop_3_apps = agg_user_app_traffic[[Social Media Traffic, Google Traffic, Email Traffic,                                   Youtube Traffic, Netflix Traffic, Gaming Traffic, Other Traffic]].sum().nlargest(3)# Plot the bar charttop_3_apps.plot(kind=bar, rot=0, color=skyblue)plt.title(Top 3 Most Used Applications)plt.xlabel(Application)plt.ylabel(Total Traffic)plt.show()  END File: note_book/user_engagement.ipynb   BEGIN File: app/main.py  Commit History: {insertions: [19], deletions: [0], lines: [19], committed_datetime: [ ], commit_count: 1}  Content: import streamlit as stfrom over_view import over_viewfrom user_engagment import engagementfrom experience_analytics import experiencdfrom satisfaction_analytics import satisfaction# Sidebar for navigationpage_options = [Over view, User Engagment, Experience Analytics, Satisfaction Analytics]selected_page = st.sidebar.selectbox(Select a page, page_options)# Display the selected pageif selected_page == Over view:    over_view()elif selected_page == User Engagment:    engagement()elif selected_page == Experience Analytics:    experiencd()elif selected_page == Satisfaction Analytics:    satisfaction()  END File: app/main.py   BEGIN File: app/other_module.py  Commit History: {insertions: [28], deletions: [0], lines: [28], committed_datetime: [ ], commit_count: 1}  Content: from sqlalchemy import create_engineimport pandas as pda function that connect to the local databasedef create_conn():    engine = None    try:        # Create an engine that connects to PostgreSQL server        engine = create_engine(postgresql://postgres:telecom@localhost:5432/telecom)        print(Connection successful)    except Exception as error:        print(error)    return enginea function that that accept engine, and table_name as an argument and return pandas data freamdef fetch_data(engine, table_name):    df = None    try:        # Execute a query and fetch all the rows into a DataFrame        df = pd.read_sql_query(fSELECT * FROM {table_name};, engine)    except Exception as error:        print(error)    return df  END File: app/other_module.py   BEGIN File: app/over_view.py  Commit History: {insertions: [15, 3], deletions: [1, 1], lines: [16, 4], committed_datetime: [ ,  ], commit_count: 2}  Content: import streamlit as stfrom other_module import fetch_data, create_connfrom visualization import create_top_10_handsets_chart, create_top_3_manufacturers_chart, create_top_5_handsets_per_manufacturer_chartdef over_view():    st.title(Over view)    st.write(This is Over view analysis TellCos company)    engine = create_conn()    data = fetch_data(engine, xdr_data)    cleaned_data = fetch_data(engine, xdr_cleaned_data)            st.write(#Top 10 Handsets)    create_top_10_handsets_chart(data)    st.write(#Top 3 Manufacturers)    create_top_3_manufacturers_chart(data)    st.write(#top_5_handsets_per_manufacturer)    create_top_5_handsets_per_manufacturer_chart(data)  END File: app/over_view.py   BEGIN File: app/visualization.py  Commit History: {insertions: [16], deletions: [11], lines: [27], committed_datetime: [ ], commit_count: 1}  Content: import streamlit as stimport plotly.express as pximport plotly.graph_objects as goimport matplotlib.pyplot as pltimport seaborn as snsimport pandas as pdfrom other_module import fetch_data, create_conn    function that plot top 10 handsets type and its countdef create_top_10_handsets_chart(data):    top_10_handsets = data[Handset Type].value_counts().head(10)    fig = px.bar(top_10_handsets, x=top_10_handsets.index, y=top_10_handsets.values, labels={x: Handset Type, y: Count})    fig.update_layout(title=Top 10 Handsets)    st.plotly_chart(fig)    function that plot top 3 manufacturers with its count def create_top_3_manufacturers_chart(data):    top_3_manufactures = data[Handset Manufacturer].value_counts().head(3)    fig = px.bar(top_3_manufactures, x=top_3_manufactures.index, y=top_3_manufactures.values, labels={x: Manufacturer, y: Count})    fig.update_layout(title=Top 3 Handset Manufacturers)    st.plotly_chart(fig)    function that plot top 5 handsets per manufacturer with its typedef create_top_5_handsets_per_manufacturer_chart(data):    top_3_manufacturers = data[Handset Manufacturer].value_counts().head(3).index    filtered_data = data[data[Handset Manufacturer].isin(top_3_manufacturers)]    top_5_handsets_per_manufacturer = filtered_data.groupby([Handset Manufacturer, Handset Type]).size().reset_index(name=Count)    fig = go.Figure(data=go.Heatmap(        z=top_5_handsets_per_manufacturer[Count],        x=top_5_handsets_per_manufacturer[Handset Manufacturer],        y=top_5_handsets_per_manufacturer[Handset Type],        colorscale=Viridis,        colorbar=dict(title=Count),    ))    fig.update_layout(title=Top 5 Handsets per Manufacturer,                      xaxis=dict(title=Manufacturer),                      yaxis=dict(title=Handset Type))    st.plotly_chart(fig)  END File: app/visualization.py   BEGIN File: note_book/User Overview analysis.ipynb  Commit History: {insertions: [61], deletions: [396], lines: [457], committed_datetime: [ ], commit_count: 1}  Content: import  script## importing data from the database engine=script.create_conn()data=script.fetch_data(engine, xdr_data)data.dtypescolumn_names_list = data.columns.tolist()print(column_names_list)## prining the first  20 handset typeprint(data[Handset Type].head(20))## pringing the top 10 handset typetop_10_handsets = data[Handset Type].value_counts().head(10)print(top_10_handsets)## prining the top 3 manufacturestop_3_manufactures = data[Handset Manufacturer].value_counts().head(3)print(top_3_manufactures)## prining the top 5 handset per manfacturer top_3_manufacturers = data[Handset Manufacturer].value_counts().head(3).indexfiltered_data = data[data[Handset Manufacturer].isin(top_3_manufacturers)]top_5_handsets_per_manufacturer = filtered_data.groupby(Handset Manufacturer)[Handset Type].value_counts().groupby(level=0, group_keys=False).nlargest(5)print(top_5_handsets_per_manufacturer)  END File: note_book/User Overview analysis.ipynb  '
